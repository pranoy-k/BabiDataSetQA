{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Supporting Fact Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import importlib, utils2; \n",
    "importlib.reload(utils2)\n",
    "from utils2 import *\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = K.tf.ConfigProto(gpu_options = {'allow_growth': True})\n",
    "K.set_session(K.tf.Session(config = cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sent):\n",
    "    return [x.strip() for x in re.split('(\\W+)?', sent) if x.strip() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_stories(lines):\n",
    "    data = []\n",
    "    story = []\n",
    "    for line in lines:\n",
    "        line = line.decode('utf-8').strip()\n",
    "        nid,line = line.split(\" \",1)\n",
    "        if int(nid) == 1:story = []\n",
    "        if '\\t' in line:\n",
    "            q, a, supporting = line.split('\\t')\n",
    "            q = tokenize(q)\n",
    "            substory = None\n",
    "            substory = [[str(i)+\":\"]+x for i,x in enumerate(story) if x]\n",
    "            data.append((substory, q, a))\n",
    "            story.append('')\n",
    "        else: story.append(tokenize(line))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = get_file('babi-tasks-v1-2.tar.gz', origin = 'https://s3.amazonaws.com/text-datasets/babi_tasks_1-20_v1-2.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar = tarfile.open(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenges = {\n",
    "    # QA1 with 10,000 samples\n",
    "    'single_supporting_fact_10k': 'tasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_{}.txt',\n",
    "    # QA2 with 10,000 samples\n",
    "    'two_supporting_facts_10k': 'tasks_1-20_v1-2/en-10k/qa2_two-supporting-facts_{}.txt',\n",
    "    'two_supporting_facts_1k': 'tasks_1-20_v1-2/en/qa2_two-supporting-facts_{}.txt',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge_type = 'single_supporting_fact_10k'\n",
    "\n",
    "challenge = challenges[challenge_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stories(file):\n",
    "    data = parse_stories(file.readlines())\n",
    "    return [(story, question, answer) for story, question, answer in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kpranoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\re.py:203: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "train_stories = get_stories(tar.extractfile(challenge.format('train')))\n",
    "test_stories = get_stories(tar.extractfile(challenge.format('test')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stories = train_stories + test_stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "story_maxlen =    max((len(x) for s,_,_ in stories for x in s))\n",
    "story_maxsents = max((len(x) for x, _, _ in stories))\n",
    "query_maxlen = max(len(x) for _, x, _ in stories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def create_vocab(stories):\n",
    "    vocab = set()\n",
    "    for i,story in enumerate(stories):\n",
    "        sys.stdout.write(\"\\r Running story number: \" + str(i))\n",
    "        \n",
    "        #Getting vocab from stories\n",
    "        for text in story[0]:\n",
    "            [vocab.add(word) for word in text ]\n",
    "        sys.stdout.flush()\n",
    "        time.sleep(0.001)\n",
    "        #getting vocab from questions\n",
    "        [vocab.add(word) for word in story[1] ]\n",
    "        \n",
    "        #Getting vocab from Answer\n",
    "        vocab.add(story[2])\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Running story number: 9700"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Running story number: 10999"
     ]
    }
   ],
   "source": [
    "vocab = sorted(create_vocab(stories))\n",
    "vocab.insert(0, '<PAD>')\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 32, 8, 4, 10000, 1000)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "story_maxsents, vocab_size, story_maxlen, query_maxlen, len(train_stories), len(test_stories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_idx = dict((c, i) for i, c in enumerate(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_stories(data, word_idx, story_maxlen, query_maxlen):\n",
    "    X = []; Xq = []; Y = []\n",
    "    for story, query, answer in data:\n",
    "        x = [[word_idx[w] for w in s] for s in story]\n",
    "        xq = [word_idx[w] for w in query]\n",
    "        y = [word_idx[answer]]\n",
    "        X.append(x); Xq.append(xq); Y.append(y)\n",
    "    return ([pad_sequences(x, maxlen=story_maxlen) for x in X],\n",
    "            pad_sequences(Xq, maxlen=query_maxlen), np.array(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_train, queries_train, answers_train = vectorize_stories(train_stories, \n",
    "     word_idx, story_maxlen, query_maxlen)\n",
    "inputs_test, queries_test, answers_test = vectorize_stories(test_stories, \n",
    "     word_idx, story_maxlen, query_maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_inputs(inputs):\n",
    "    for i,it in enumerate(inputs):\n",
    "        inputs[i] = np.concatenate([it, \n",
    "                           np.zeros((story_maxsents-it.shape[0],story_maxlen), 'int')])\n",
    "    return np.stack(inputs)\n",
    "inputs_train = stack_inputs(inputs_train)\n",
    "inputs_test = stack_inputs(inputs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 10, 8), (1000, 10, 8))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_train.shape, inputs_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "inps = [inputs_train, queries_train]\n",
    "val_inps = [inputs_test, queries_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 20\n",
    "parms = {'verbose': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emb_sent_bow(inp):\n",
    "    emb = TimeDistributed(Embedding(vocab_size, emb_dim))(inp)\n",
    "    return Lambda(lambda x: K.sum(x, 2))(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([Dimension(None), Dimension(10), Dimension(8)]),\n",
       " TensorShape([Dimension(None), Dimension(10), Dimension(20)]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_story = Input((story_maxsents, story_maxlen))\n",
    "emb_story = emb_sent_bow(inp_story)\n",
    "inp_story.get_shape(), emb_story.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_q = Input((query_maxlen,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([Dimension(None), Dimension(4)]),\n",
       " TensorShape([Dimension(None), Dimension(1), Dimension(20)]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_q = Embedding(vocab_size, emb_dim)(inp_q)\n",
    "emb_q = Lambda(lambda x: K.sum(x, 1))(emb_q)\n",
    "emb_q = Reshape((1, emb_dim))(emb_q)\n",
    "inp_q.get_shape(), emb_q.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kpranoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\kpranoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\legacy\\layers.py:458: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(10), Dimension(1)])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = merge([emb_story, emb_q], mode='dot', dot_axes=2)\n",
    "x = Reshape((story_maxsents,))(x)\n",
    "x = Activation('softmax')(x)\n",
    "match = Reshape((story_maxsents,1))(x)\n",
    "match.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kpranoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  \n",
      "C:\\Users\\kpranoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\legacy\\layers.py:458: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n"
     ]
    }
   ],
   "source": [
    "emb_c = emb_sent_bow(inp_story)\n",
    "x = merge([match,emb_c], mode='dot', dot_axes=1)\n",
    "response = Reshape((emb_dim,),)(x)\n",
    "res = Dense(vocab_size, activation= 'softmax')(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = Model([inp_story, inp_q], res)\n",
    "answer.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kpranoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/4\n",
      "0s - loss: 0.3701 - acc: 0.8794 - val_loss: 2.2982e-04 - val_acc: 1.0000\n",
      "Epoch 2/4\n",
      "0s - loss: 0.0055 - acc: 0.9979 - val_loss: 0.1486 - val_acc: 0.9750\n",
      "Epoch 3/4\n",
      "0s - loss: 0.0062 - acc: 0.9986 - val_loss: 5.0378e-06 - val_acc: 1.0000\n",
      "Epoch 4/4\n",
      "0s - loss: 0.0140 - acc: 0.9976 - val_loss: 0.0265 - val_acc: 0.9960\n"
     ]
    }
   ],
   "source": [
    "K.set_value(answer.optimizer.lr, 1e-2)\n",
    "hist = answer.fit(inps, answers_train, **parms, nb_epoch=4, batch_size=32,\n",
    "                 validation_data=(val_inps, answers_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_num = 117\n",
    "f = Model([inp_story, inp_q],match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['0:', 'Mary', 'went', 'back', 'to', 'the', 'hallway', '.'],\n",
       "  ['1:', 'Daniel', 'went', 'back', 'to', 'the', 'bedroom', '.'],\n",
       "  ['3:', 'Sandra', 'moved', 'to', 'the', 'bathroom', '.'],\n",
       "  ['4:', 'Sandra', 'journeyed', 'to', 'the', 'hallway', '.'],\n",
       "  ['6:', 'Mary', 'went', 'back', 'to', 'the', 'bedroom', '.'],\n",
       "  ['7:', 'Mary', 'went', 'back', 'to', 'the', 'garden', '.']],\n",
       " ['Where', 'is', 'Mary', '?'],\n",
       " 'garden')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stories[q_num]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to Predict which sentence has the highest weight on a pretrained model for better understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.4759e-07,   7.7565e-13,   9.0063e-11,   4.4669e-10,\n",
       "         9.2971e-02,   9.0703e-01,   1.2456e-18], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.squeeze(f.predict([inputs_train[q_num:q_num+1],queries_train[q_num:q_num+1\n",
    "                                             ]]))[:(len(train_stories[q_num][0]) + 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now applying it to our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([21, 27, 27, 22, 25, 20, 20, 20, 25, 19])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers_train[q_num:q_num+10,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([21, 27, 27, 22, 25, 20, 20, 20, 25, 19], dtype=int64)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(answer.predict([inputs_train[q_num:q_num+10], queries_train[q_num:q_num+10]]),1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two Supporting Facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['0:', 'Mary', 'moved', 'to', 'the', 'office', '.'],\n",
       "  ['1:', 'John', 'moved', 'to', 'the', 'garden', '.'],\n",
       "  ['3:', 'Sandra', 'moved', 'to', 'the', 'bedroom', '.'],\n",
       "  ['4:', 'Sandra', 'went', 'back', 'to', 'the', 'office', '.'],\n",
       "  ['6:', 'John', 'went', 'to', 'the', 'bedroom', '.'],\n",
       "  ['7:', 'John', 'journeyed', 'to', 'the', 'garden', '.'],\n",
       "  ['9:', 'Daniel', 'went', 'back', 'to', 'the', 'hallway', '.'],\n",
       "  ['10:', 'John', 'journeyed', 'to', 'the', 'bedroom', '.'],\n",
       "  ['12:', 'Daniel', 'journeyed', 'to', 'the', 'bathroom', '.'],\n",
       "  ['13:', 'John', 'travelled', 'to', 'the', 'garden', '.']],\n",
       " ['Where', 'is', 'Daniel', '?'],\n",
       " 'bathroom')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_stories[534]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
